
## 目的
ユーザーが入力した検索語句に応じてベクトル検索を行えるようにすること

## 本サーバーの機能
サイト上からチャンキングした文章のベクトル化、およびにユーザーが入力した検索語句のベクトル化

## 他サーバーで行われる機能
- ベクトル化された文章を保存
- ベクトル DB からの検索

# ベクトル検索の基本
ベクトルのコサイン類似度の計算はベクトル DB の機能を利用するのが高率的。わざわざアプリ側で計算しない。
保存したベクトルをさらに加工して、次元を落とすことでRDBにおけるインデックスと似た効果を持たせる技術も登場している。

# ベクトル DB の選定
ベクトルに特化したDBも存在はするものの、発展途上であり、小規模においてはRDBを利用するのが現実的である。
https://zenn.dev/rwcolinpeng/articles/45632994cf8bc1

# 計算インフラを考慮したアーキテクチャ選定
1. 通常の EC2, ECS 上で自然言語処理を行う（某インフラ構成と同様）
2. Lambda などのサーバーレスで自然言語処理を行う
   1. パターン1: そのまま自然言語処理を行う（Python, SentenceTransformer, モデルにより重いコールドスタートが発生）
   2. パターン2: ONNX などのコンテナを利用して、モデルを事前にロードしておくことでコールドスタートを回避する
3. SageMaker などのマネージドサービスを利用して自然言語処理を行う

# ONNX
ONNX（Open Neural Network Exchange）は、異なるフレームワーク間でモデルを共有するためのオープンなフォーマットです。これにより、PyTorchやTensorFlowなどの異なるフレームワークでトレーニングされたモデルを、他のフレームワークで推論に使用できます。

# SageMaker
SageMaker が EC2 などと比較して優れている点は以下の通りです：

運用・管理の簡素化
自動スケーリング:

トラフィックに応じた自動的なインスタンス追加・削除
EC2では手動設定が必要なオートスケーリンググループが自動構築
マネージドインフラ:

OS、Python環境、機械学習ライブラリの管理が不要
セキュリティパッチやアップデートが自動適用

# SageMakerでのモデルデプロイ例
predictor = model.deploy(
    initial_instance_count=1,
    instance_type='ml.t3.medium',
    auto_scaling_enabled=True
)